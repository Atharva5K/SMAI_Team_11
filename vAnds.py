# -*- coding: utf-8 -*-
"""NLP_V&S_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CDZMXqnOVsyLIUdLWGHKrK7x8zss_MMP
"""

from transformers import BertTokenizer, BertForMaskedLM
from transformers import pipeline
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from gensim.models import Word2Vec
from nltk.sentiment.vader import SentimentIntensityAnalyzer
import nltk
import os
import sys


sys.path.append(".")  # Ensure current directory is in path
try:
    from Code.commentary_classifier import CommentaryClassifier, download_nltk_data
except ImportError:
    from commentary_classifier import CommentaryClassifier, download_nltk_data

# Download NLTK data if not already available
def download_nltk_data():
    try:
        nltk.data.find('tokenizers/punkt')
        nltk.data.find('corpora/stopwords')
        nltk.data.find('corpora/wordnet')
        nltk.data.find('sentiment/vader_lexicon')
    except LookupError:
        print("Downloading required NLTK data...")
        nltk.download('punkt')
        nltk.download('stopwords')
        nltk.download('wordnet')
        nltk.download('vader_lexicon')


download_nltk_data()

# Load pre-trained BERT model and tokenizer
print("Loading BERT model and tokenizer...")
model_name = "bert-base-uncased"
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertForMaskedLM.from_pretrained(model_name)

# Define the summarization pipeline
print("Setting up summarization pipeline...")
summarization_pipeline = pipeline("summarization")


def summarize_over(over):
    print(f"Summarizing over with {len(over)} commentary lines")
    over_text = " ".join(over)
    summary = summarization_pipeline(over_text, max_length=100, min_length=10, do_sample=False)[0]['summary_text']
    return summary


def highlights(text_lines, n):
    print(f"Finding highlights for {len(text_lines)} overs, top {n}")
    interesting_scores = calculate_interesting_scores(text_lines)
    sorted_scores = sorted(interesting_scores.items(), key=lambda x: x[1], reverse=True)
    top_n_overs = dict(sorted_scores[:n])
    highlights = {}
    for over_number in top_n_overs.keys():
        over_text = text_lines[over_number - 1]
        summary = summarize_over(over_text)
        highlights[over_number] = summary
    return highlights


def calculate_interesting_scores(text_lines):
    print("Calculating interesting scores...")
    stop_words = set(stopwords.words('english'))
    lemmatizer = WordNetLemmatizer()

    def preprocess_text(text_list):
        preprocessed_text = []
        for text in text_list:
            tokens = word_tokenize(text.lower())
            tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalnum() and token not in stop_words]
            preprocessed_text.append(tokens)
        return preprocessed_text

    def wordscore(text_lines):
        print("Calculating word scores...")
        scores = {}
        for i, sublist in enumerate(text_lines):
            preprocessed_commentary = preprocess_text(sublist)
            word2vec_model = Word2Vec(preprocessed_commentary, vector_size=100, window=5, min_count=1, workers=4)
            cricket_related_terms = ["six", "four", "boundary", "wicket", "catch", "stump"]
            related_words = {}
            for term in cricket_related_terms:
                if term in word2vec_model.wv.key_to_index:
                    similar_words = word2vec_model.wv.most_similar(term, topn=5)
                    related_words[term] = [word for word, _ in similar_words]
            count_related_words = sum(len(words) for words in related_words.values())
            scores[i + 1] = count_related_words
        return scores

    def sentimentscore(text_lines):
        print("Calculating sentiment scores...")
        sia = SentimentIntensityAnalyzer()
        over_sentiment_scores = {}
        for over_number, over in enumerate(text_lines, start=1):
            over_text = ' '.join(over)
            sentiment_score = sia.polarity_scores(over_text)["compound"]
            over_sentiment_scores[over_number] = sentiment_score
        return over_sentiment_scores

    sentiment_scores = sentimentscore(text_lines)
    word_scores = wordscore(text_lines)

    min_sentiment = min(sentiment_scores.values())
    max_sentiment = max(sentiment_scores.values())
    sentiment_range = max_sentiment - min_sentiment

    # Avoid division by zero
    if sentiment_range == 0:
        normalized_sentiments = {over: 0.5 for over in sentiment_scores.keys()}
    else:
        normalized_sentiments = {over: (score - min_sentiment) / sentiment_range
                                 for over, score in sentiment_scores.items()}

    min_word_score = min(word_scores.values())
    max_word_score = max(word_scores.values())
    word_score_range = max_word_score - min_word_score

    # Avoid division by zero
    if word_score_range == 0:
        normalized_word_scores = {over: 0.5 for over in word_scores.keys()}
    else:
        normalized_word_scores = {over: (score - min_word_score) / word_score_range
                                  for over, score in word_scores.items()}

    interesting_scores = {over: normalized_sentiments[over] + normalized_word_scores[over]
                          for over in sentiment_scores.keys()}

    return interesting_scores


# def v_and_s(text_lines, n):
#     print(f"Analyzing {len(text_lines)} overs for highlights...")
#     interesting_scores = calculate_interesting_scores(text_lines)
#     highlighted_overs = highlights(text_lines, n)
#     sorted_highlights = sorted(highlighted_overs.items(), key=lambda x: interesting_scores[x[0]], reverse=True)
#     print("\nHighlighted Overs:")
#     for over, summary in sorted_highlights:
#         print(f"Over {over}: {summary}\n")
#     print("Analysis complete!")

def v_and_s(text_lines, n, include_classification=True):
    print(f"Analyzing {len(text_lines)} overs for highlights...")

    # Original interesting scores calculation
    interesting_scores = calculate_interesting_scores(text_lines)
    highlighted_overs = highlights(text_lines, n)
    sorted_highlights = sorted(highlighted_overs.items(), key=lambda x: interesting_scores[x[0]], reverse=True)

    # Print highlights
    print("\nHighlighted Overs:")
    for over, summary in sorted_highlights:
        print(f"Over {over}: {summary}\n")

    # Add zero-shot classification if requested
    if include_classification:
        try:
            # Initialize the classifier
            classifier = CommentaryClassifier()

            # Perform classification
            classifications = classifier.batch_classify_overs(text_lines)

            # Analyze results
            analysis = classifier.analyze_category_distribution(classifications)

            # Print the report
            classifier.print_classification_report(analysis)

            # Return both highlights and classifications
            return {
                'highlights': dict(sorted_highlights),
                'classifications': classifications,
                'analysis': analysis
            }
        except Exception as e:
            print(f"Error performing classification: {e}")
            print("Continuing with basic analysis...")

    print("Analysis complete!")
    return dict(sorted_highlights)